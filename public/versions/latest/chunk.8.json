{"core/testing_examples/modules/hb_ao_test_vectors.md":{"content":"# Module: hb_ao_test_vectors\n\n## Basic Information\n- **Source File:** hb_ao_test_vectors.erl\n- **Module Type:** Testing Example\n- **Purpose:** Resolution Engine Test Vectors\n\n## Purpose\nProvides a comprehensive suite of test vectors for validating the resolution engine's execution under different circumstances. The module tests various aspects of message resolution, device handling, and state management with different configuration options.\n\n## Interface\n\n### Core Operations\n- `run_test/0` - Run single test via command line\n- `run_all_test_/0` - Run all test vectors\n- `test_suite/0` - Define test suite\n- `test_opts/0` - Define test configurations\n\n## Dependencies\n\n### Direct Dependencies\n- eunit: Testing framework\n- hb_test_utils: Test utilities\n- hb_ao: Resolution engine\n- hb_message: Message handling\n- hb_cache: Cache operations\n\n### Inverse Dependencies\n- System testing\n- Resolution validation\n- Device testing\n\n## Implementation Details\n\n### Key Concepts\n\n1. **Test Configuration**\n   ```erlang\n   % Test options configuration\n   test_opts() ->\n       [\n           #{\n               name => no_cache,\n               desc => \"No cache read or write\",\n               opts => #{\n                   hashpath => ignore,\n                   cache_control => [<<\"no-cache\">>, <<\"no-store\">>],\n                   spawn_worker => false,\n                   store => #{\n                       <<\"store-module\">> => hb_store_fs,\n                       <<\"prefix\">> => <<\"cache-TEST/fs\">>\n                   }\n               },\n               skip => [load_as]\n           }\n       ]\n   ```\n   Features:\n   - Multiple configurations\n   - Cache control\n   - Store settings\n   - Test filtering\n\n2. **Test Suite Organization**\n   ```erlang\n   % Test suite structure\n   test_suite() ->\n       [\n           {resolve_simple, \"resolve simple\",\n               fun resolve_simple_test/1},\n           {resolve_id, \"resolve id\",\n               fun resolve_id_test/1}\n           % ... more tests\n       ]\n   ```\n   Provides:\n   - Test grouping\n   - Test descriptions\n   - Function mapping\n   - Skip handling\n\n3. **Device Testing**\n   ```erlang\n   % Device test configuration\n   generate_device_with_keys_using_args() ->\n       #{\n           key_using_only_state =>\n               fun(State) ->\n                   {ok, <<(maps:get(<<\"state_key\">>, State))/binary>>}\n               end,\n           key_using_state_and_msg =>\n               fun(State, Msg) ->\n                   {ok, <<(maps:get(<<\"state_key\">>, State))/binary,\n                         (maps:get(<<\"msg_key\">>, Msg))/binary>>}\n               end\n       }\n   ```\n   Shows:\n   - Device creation\n   - State handling\n   - Message handling\n   - Function arity\n\n### Test Categories\n\n1. **Resolution Tests**\n   ```erlang\n   % Basic resolution test\n   resolve_simple_test(Opts) ->\n       Res = hb_ao:resolve(#{ <<\"a\">> => <<\"RESULT\">> }, <<\"a\">>, Opts),\n       ?assertEqual({ok, <<\"RESULT\">>}, Res).\n   ```\n   Tests:\n   - Simple resolution\n   - Key handling\n   - Value retrieval\n   - Error handling\n\n2. **Device Tests**\n   ```erlang\n   % Device function test\n   device_with_handler_function_test(Opts) ->\n       Msg = #{\n           device => gen_handler_device(),\n           test_key => <<\"BAD\">>\n       },\n       ?assertEqual(\n           {ok, <<\"HANDLER VALUE\">>},\n           hb_ao:resolve(Msg, <<\"test_key\">>, Opts)\n       ).\n   ```\n   Verifies:\n   - Device loading\n   - Function handling\n   - State management\n   - Error cases\n\n### Test Configurations\n\n1. **Cache Control**\n   ```erlang\n   % Cache configuration\n   #{\n       name => no_cache,\n       desc => \"No cache read or write\",\n       opts => #{\n           hashpath => ignore,\n           cache_control => [<<\"no-cache\">>, <<\"no-store\">>]\n       }\n   }\n   ```\n   Manages:\n   - Cache settings\n   - Store options\n   - Path handling\n   - Worker control\n\n2. **Store Settings**\n   ```erlang\n   % Store configuration\n   store => #{\n       <<\"store-module\">> => hb_store_fs,\n       <<\"prefix\">> => <<\"cache-TEST/fs\">>\n   }\n   ```\n   Controls:\n   - Store module\n   - Path prefix\n   - File system\n   - Cache location\n\n## Integration Points\n\n1. **Resolution System**\n   - Message resolution\n   - Key handling\n   - Value retrieval\n   - Error management\n\n2. **Device System**\n   - Device loading\n   - Function execution\n   - State management\n   - Error handling\n\n3. **Cache System**\n   - Cache control\n   - Store operations\n   - Path handling\n   - Worker management\n\n## Analysis Insights\n\n### Performance Considerations\n\n1. **Resource Management**\n   - Store initialization\n   - Cache control\n   - Worker handling\n   - Memory usage\n\n2. **Operation Efficiency**\n   - Resolution speed\n   - Cache usage\n   - Store access\n   - Worker allocation\n\n### Security Implications\n\n1. **Device Safety**\n   - Function validation\n   - State protection\n   - Error handling\n   - Resource cleanup\n\n2. **Store Safety**\n   - Path validation\n   - Access control\n   - Error handling\n   - Resource protection\n\n### Best Practices\n\n1. **Test Organization**\n   ```erlang\n   % Recommended test structure\n   organize_test() ->\n       % 1. Setup\n       setup_test_environment(),\n       \n       % 2. Execute\n       run_test_vectors(),\n       \n       % 3. Verify\n       verify_results().\n   ```\n\n2. **Device Testing**\n   ```erlang\n   % Recommended device test\n   test_device() ->\n       % 1. Create device\n       Device = create_test_device(),\n       \n       % 2. Execute operations\n       Results = execute_operations(Device),\n       \n       % 3. Verify behavior\n       verify_device_behavior(Results).\n   ```\n\n3. **Configuration Management**\n   ```erlang\n   % Recommended config handling\n   manage_config() ->\n       % 1. Setup config\n       Config = setup_config(),\n       \n       % 2. Run tests\n       Results = run_with_config(Config),\n       \n       % 3. Cleanup\n       cleanup_config(Config).\n   ```\n\n### Example Usage\n\n```erlang\n% Run single test\nhb_test_utils:run(start_as, normal, test_suite(), test_opts()),\n\n% Run all tests\nhb_test_utils:suite_with_opts(test_suite(), test_opts()),\n\n% Test with specific device\nDevice = generate_device_with_keys_using_args(),\nMsg = #{ device => Device, state_key => <<\"1\">> },\nhb_ao:resolve(Msg, #{ path => <<\"key_using_only_state\">> }, Opts)\n```\n\n## Test Flow\n\n```mermaid\ngraph TD\n    A[Test Suite] -->|Configure| B[Test Options]\n    B -->|Execute| C[Test Cases]\n    C -->|Validate| D[Results]\n    \n    D -->|Success| E[Pass]\n    D -->|Failure| F[Fail]\n    \n    E --> G[Next Test]\n    F --> G\n```\n\n## Device Test Flow\n\n```mermaid\nsequenceDiagram\n    participant Test as Test Suite\n    participant Device as Device\n    participant State as State\n    participant Result as Result\n\n    Test->>Device: Create Device\n    Test->>State: Initialize State\n    \n    loop Test Cases\n        Test->>Device: Execute Function\n        Device->>State: Access State\n        State-->>Device: State Data\n        Device-->>Test: Result\n        Test->>Result: Verify\n    end\n"},"core/testing_examples/modules/hb_examples.md":{"content":"# Module: hb_examples\n\n## Basic Information\n- **Source File:** hb_examples.erl\n- **Module Type:** Testing Example\n- **Purpose:** End-to-End Test Examples\n\n## Purpose\nProvides comprehensive end-to-end test examples that demonstrate how to interact with HyperBEAM nodes through the HTTP interface. These tests serve both as system validation and as usage examples for developers.\n\n## Interface\n\n### Core Tests\n- `relay_with_payments_test/0` - Message relay with payment system\n- `paid_wasm_test/0` - WASM execution with payment\n- `create_schedule_aos2_test_disabled/0` - Process scheduling (disabled)\n- `schedule/2,3,4` - Helper functions for scheduling\n\n## Dependencies\n\n### Direct Dependencies\n- eunit: Testing framework\n- hb_http: HTTP client\n- hb_message: Message handling\n- ar_wallet: Arweave wallet operations\n- hb_util: Utility functions\n\n### Inverse Dependencies\n- Used as examples\n- System testing\n- Documentation\n\n## Implementation Details\n\n### Key Concepts\n\n1. **Payment Integration**\n   ```erlang\n   % Payment system setup\n   HostNode =\n       hb_http_server:start_node(\n           #{\n               operator => ar_wallet:to_address(HostWallet),\n               preprocessor => ProcessorMsg,\n               postprocessor => ProcessorMsg\n           }\n       ).\n   ```\n   Features:\n   - Wallet management\n   - Payment processing\n   - Balance tracking\n   - Error handling\n\n2. **Message Relay**\n   ```erlang\n   % Message relay with payment\n   ClientMessage =\n       hb_message:commit(\n           #{\n               <<\"path\">> => <<\"/~relay@1.0/call?relay-path=https://www.google.com\">>\n           },\n           ClientWallet\n       ),\n   Res = hb_http:get(HostNode, ClientMessage, #{})\n   ```\n   Demonstrates:\n   - Message creation\n   - HTTP integration\n   - Payment handling\n   - Error handling\n\n3. **WASM Integration**\n   ```erlang\n   % WASM execution\n   {ok, WASMFile} = file:read_file(<<\"test/test-64.wasm\">>),\n   ClientMessage =\n       hb_message:commit(\n           #{\n               <<\"path\">> => <<\"/~wasm-64@1.0/init/compute/results?function=fac\">>,\n               <<\"body\">> => WASMFile,\n               <<\"parameters+list\">> => <<\"3.0\">>\n           },\n           ClientWallet\n       )\n   ```\n   Shows:\n   - WASM loading\n   - Function execution\n   - Parameter passing\n   - Result handling\n\n### Test Scenarios\n\n1. **Payment Flow**\n   ```erlang\n   % Payment test flow\n   % 1. Try without balance\n   Res1 = hb_http:get(HostNode, ClientMessage1, #{}),\n   ?assertMatch({error, #{ <<\"body\">> := <<\"Insufficient funds\">> }}, Res1),\n   \n   % 2. Topup balance\n   TopupMessage =\n       hb_message:commit(\n           #{\n               <<\"path\">> => <<\"/~simple-pay@1.0/topup\">>,\n               <<\"recipient\">> => ClientAddress,\n               <<\"amount\">> => 100\n           },\n           HostWallet\n       ),\n   \n   % 3. Try again with balance\n   Res2 = hb_http:get(HostNode, ClientMessage1, #{})\n   ```\n   Tests:\n   - Balance checking\n   - Payment processing\n   - Error handling\n   - Success validation\n\n2. **WASM Execution**\n   ```erlang\n   % WASM test flow\n   % 1. Execute WASM\n   {ok, Res} = hb_http:post(HostNode, ClientMessage1, #{}),\n   \n   % 2. Verify signature\n   ?assert(length(hb_message:signers(Res)) > 0),\n   ?assert(hb_message:verify(Res)),\n   \n   % 3. Check result\n   ?assertMatch(6.0, hb_ao:get(<<\"output/1\">>, Res, #{}))\n   ```\n   Verifies:\n   - WASM execution\n   - Result validation\n   - Signature checking\n   - Balance deduction\n\n### Process Management\n\n1. **Process Creation**\n   ```erlang\n   % Process configuration\n   ProcMsg = #{\n       <<\"data-protocol\">> => <<\"ao\">>,\n       <<\"type\">> => <<\"Process\">>,\n       <<\"variant\">> => <<\"ao.TN.1\">>,\n       <<\"module\">> => ModuleID,\n       <<\"scheduler\">> => SchedulerID,\n       <<\"memory-limit\">> => <<\"1024-mb\">>,\n       <<\"compute-limit\">> => <<\"10000000\">>\n   }\n   ```\n   Manages:\n   - Process specs\n   - Resource limits\n   - Scheduler config\n   - Authority setup\n\n2. **Scheduling**\n   ```erlang\n   % Schedule process\n   schedule(ProcMsg, Target, Wallet, Node) ->\n       SignedReq = \n           hb_message:commit(\n               #{\n                   <<\"path\">> => <<\"/~scheduler@1.0/schedule\">>,\n                   <<\"target\">> => Target,\n                   <<\"body\">> => ProcMsg\n               },\n               Wallet\n           ),\n       hb_http:post(Node, SignedReq, #{})\n   ```\n   Handles:\n   - Process scheduling\n   - Message signing\n   - Request sending\n   - Response handling\n\n## Integration Points\n\n1. **HTTP System**\n   - Request sending\n   - Response handling\n   - Error management\n   - Status tracking\n\n2. **Payment System**\n   - Balance management\n   - Payment processing\n   - Error handling\n   - State tracking\n\n3. **WASM System**\n   - Code execution\n   - Result handling\n   - Error management\n   - Resource tracking\n\n## Analysis Insights\n\n### Performance Considerations\n\n1. **Resource Management**\n   - Memory limits\n   - Compute limits\n   - Balance tracking\n   - Error handling\n\n2. **Operation Efficiency**\n   - Quick validation\n   - Fast execution\n   - Clean termination\n   - Resource cleanup\n\n### Security Implications\n\n1. **Message Safety**\n   - Signature validation\n   - Balance checking\n   - Error handling\n   - State protection\n\n2. **Process Safety**\n   - Resource limits\n   - Authority checking\n   - Error handling\n   - State protection\n\n### Best Practices\n\n1. **Test Organization**\n   ```erlang\n   % Recommended test structure\n   test_flow() ->\n       % 1. Setup\n       setup_environment(),\n       \n       % 2. Execute\n       Result = execute_operation(),\n       \n       % 3. Verify\n       verify_result(Result).\n   ```\n\n2. **Error Handling**\n   ```erlang\n   % Recommended error handling\n   handle_operation() ->\n       try\n           execute_operation()\n       catch\n           Error ->\n               handle_error(Error)\n       end.\n   ```\n\n3. **Resource Management**\n   ```erlang\n   % Recommended resource handling\n   manage_resources() ->\n       Resources = allocate_resources(),\n       try\n           use_resources(Resources)\n       after\n           cleanup_resources(Resources)\n       end.\n   ```\n\n### Example Usage\n\n```erlang\n% Start test node\nHostNode = hb_http_server:start_node(#{\n    operator => OperatorAddress,\n    preprocessor => ProcessorConfig,\n    postprocessor => ProcessorConfig\n}),\n\n% Create and send message\nMessage = hb_message:commit(\n    #{\n        <<\"path\">> => <<\"/test/path\">>,\n        <<\"body\">> => <<\"test data\">>\n    },\n    Wallet\n),\n{ok, Response} = hb_http:post(HostNode, Message, #{})\n```\n\n## Test Flow\n\n```mermaid\ngraph TD\n    A[Setup Environment] -->|Configure| B[Create Message]\n    B -->|Send| C[Process Request]\n    C -->|Validate| D[Check Result]\n    \n    D -->|Success| E[Verify State]\n    D -->|Failure| F[Handle Error]\n    \n    E --> G[Cleanup]\n    F --> G\n```\n\n## Message Flow\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Host\n    participant Payment\n    participant WASM\n\n    Client->>Host: Send Request\n    Host->>Payment: Check Balance\n    Payment-->>Host: Balance OK\n    Host->>WASM: Execute Code\n    WASM-->>Host: Result\n    Host->>Payment: Deduct Balance\n    Host-->>Client: Signed Response\n"},"core/testing_examples/modules/hb_http_benchmark_tests.md":{"content":"# Module: hb_http_benchmark_tests\n\n## Basic Information\n- **Source File:** hb_http_benchmark_tests.erl\n- **Module Type:** Performance Testing\n- **Purpose:** HTTP Performance Benchmarking\n\n## Purpose\nProvides comprehensive performance benchmarks for HTTP operations in HyperBEAM, including message resolution, WASM execution, and process scheduling. The module uses a performance divider to adjust expectations based on machine specifications.\n\n## Interface\n\n### Core Tests (Currently Disabled)\n- `unsigned_resolve_benchmark_test/0` - Benchmark unsigned message resolution\n- `parallel_unsigned_resolve_benchmark_test/0` - Parallel unsigned resolution\n- `run_wasm_unsigned_benchmark_test/0` - WASM execution benchmark\n- `run_wasm_signed_benchmark_test_disabled/0` - Signed WASM execution\n- `parallel_wasm_unsigned_benchmark_test_disabled/0` - Parallel WASM execution\n- `parallel_wasm_signed_benchmark_test_disabled/0` - Parallel signed WASM\n- `parallel_http_scheduling_benchmark_test/0` - Process scheduling benchmark\n\n## Dependencies\n\n### Direct Dependencies\n- eunit: Testing framework\n- hb_http: HTTP client\n- hb_http_server: Server operations\n- hb_util: Utility functions\n- hb: Core benchmarking\n\n### Inverse Dependencies\n- Performance testing\n- System validation\n- Capacity planning\n\n## Implementation Details\n\n### Key Concepts\n\n1. **Performance Scaling**\n   ```erlang\n   % Performance adjustment\n   -define(PERFORMANCE_DIVIDER, 1).\n   % 1: 50% performance of Macbook Pro M2 Max\n   ```\n   Features:\n   - Machine scaling\n   - Performance targets\n   - Benchmark adjustment\n   - Test calibration\n\n2. **WASM Testing**\n   ```erlang\n   % WASM test configuration\n   wasm_compute_request(ImageFile, Func, Params) ->\n       {ok, Bin} = file:read_file(ImageFile),\n       #{\n           <<\"path\">> => <<\"init/compute/results\">>,\n           <<\"device\">> => <<\"wasm-64@1.0\">>,\n           <<\"function\">> => Func,\n           <<\"parameters\">> => Params,\n           <<\"image\">> => Bin\n       }.\n   ```\n   Demonstrates:\n   - WASM loading\n   - Function setup\n   - Parameter handling\n   - Request formatting\n\n3. **Parallel Testing**\n   ```erlang\n   % Parallel benchmark setup\n   BenchTime = 1,\n   BenchWorkers = 16,\n   URL = hb_http_server:start_node(#{force_signed => false}),\n   Iterations = hb:benchmark(\n       fun(_Count) -> execute_operation() end,\n       BenchTime,\n       BenchWorkers\n   )\n   ```\n   Shows:\n   - Worker management\n   - Time tracking\n   - Result collection\n   - Performance measurement\n\n### Benchmark Types\n\n1. **Message Resolution**\n   ```erlang\n   % Message resolution benchmark\n   hb_http:post(URL,\n       #{\n           <<\"path\">> => <<\"key1\">>,\n           <<\"key1\">> => #{<<\"key2\">> => <<\"value1\">>}\n       },\n       #{}\n   )\n   ```\n   Tests:\n   - Message handling\n   - Path resolution\n   - Response time\n   - System throughput\n\n2. **WASM Execution**\n   ```erlang\n   % WASM execution benchmark\n   case hb_http:post(URL, Msg, #{}) of\n       {ok, _} -> 1;\n       _ -> 0\n   end\n   ```\n   Verifies:\n   - WASM loading\n   - Function execution\n   - Result handling\n   - Error management\n\n### Performance Metrics\n\n1. **Throughput Measurement**\n   ```erlang\n   % Performance reporting\n   hb_util:eunit_print(\n       \"Resolved ~p messages via HTTP (~p workers) in ~p seconds (~.2f msg/s)\",\n       [Iterations, BenchWorkers, BenchTime, Iterations / BenchTime]\n   )\n   ```\n   Tracks:\n   - Message count\n   - Time taken\n   - Worker count\n   - Messages per second\n\n2. **Performance Validation**\n   ```erlang\n   % Performance assertion\n   ?assert(Iterations > 400 / ?PERFORMANCE_DIVIDER)\n   ```\n   Ensures:\n   - Minimum throughput\n   - Scaled targets\n   - System capacity\n   - Performance goals\n\n## Integration Points\n\n1. **HTTP System**\n   - Request handling\n   - Response timing\n   - Error management\n   - Performance tracking\n\n2. **WASM System**\n   - Code execution\n   - Resource usage\n   - Error handling\n   - Performance metrics\n\n3. **Process System**\n   - Scheduling operations\n   - Resource allocation\n   - State management\n   - Performance monitoring\n\n## Analysis Insights\n\n### Performance Considerations\n\n1. **Resource Usage**\n   - Worker allocation\n   - Memory management\n   - CPU utilization\n   - Network bandwidth\n\n2. **Operation Efficiency**\n   - Request handling\n   - Response timing\n   - Resource cleanup\n   - Error recovery\n\n### Security Implications\n\n1. **Message Safety**\n   - Signed messages\n   - Unsigned messages\n   - Validation checks\n   - Error handling\n\n2. **Resource Safety**\n   - Worker limits\n   - Memory bounds\n   - CPU constraints\n   - Network limits\n\n### Best Practices\n\n1. **Benchmark Organization**\n   ```erlang\n   % Recommended benchmark structure\n   benchmark_operation() ->\n       % 1. Setup\n       setup_environment(),\n       \n       % 2. Execute\n       Results = run_benchmark(),\n       \n       % 3. Validate\n       validate_results(Results).\n   ```\n\n2. **Performance Testing**\n   ```erlang\n   % Recommended test pattern\n   test_performance() ->\n       % 1. Configure\n       configure_benchmark(),\n       \n       % 2. Execute\n       Results = execute_benchmark(),\n       \n       % 3. Assert\n       assert_performance(Results).\n   ```\n\n3. **Resource Management**\n   ```erlang\n   % Recommended resource handling\n   manage_resources() ->\n       Resources = allocate_resources(),\n       try\n           use_resources(Resources)\n       after\n           cleanup_resources(Resources)\n       end.\n   ```\n\n### Example Usage\n\n```erlang\n% Configure benchmark\nURL = hb_http_server:start_node(#{\n    force_signed => false\n}),\n\n% Run benchmark\nIterations = hb:benchmark(\n    fun() ->\n        hb_http:post(URL,\n            #{<<\"path\">> => <<\"test\">>},\n            #{}\n        )\n    end,\n    BenchTime\n),\n\n% Validate results\n?assert(Iterations > MinimumExpected / ?PERFORMANCE_DIVIDER)\n```\n\n## Benchmark Flow\n\n```mermaid\ngraph TD\n    A[Setup Environment] -->|Configure| B[Start Server]\n    B -->|Initialize| C[Create Workers]\n    C -->|Execute| D[Run Tests]\n    \n    D -->|Collect| E[Gather Results]\n    E -->|Analyze| F[Calculate Metrics]\n    F -->|Validate| G[Assert Performance]\n```\n\n## Test Execution Flow\n\n```mermaid\nsequenceDiagram\n    participant Test as Benchmark\n    participant Server as HTTP Server\n    participant Workers as Worker Pool\n    participant System as System Under Test\n\n    Test->>Server: Start Server\n    Test->>Workers: Create Workers\n    \n    loop Benchmark Duration\n        Workers->>Server: Send Requests\n        Server->>System: Process\n        System-->>Server: Response\n        Server-->>Workers: Results\n    end\n    \n    Workers->>Test: Report Metrics\n    Test->>Test: Validate Performance\n"},"core/testing_examples/modules/hb_test_utils.md":{"content":"# Module: hb_test_utils\n\n## Basic Information\n- **Source File:** hb_test_utils.erl\n- **Module Type:** Testing Utility\n- **Purpose:** Test Suite Management\n\n## Purpose\nProvides utilities for running test suites with different configurations in HyperBEAM. The module enables running tests with various store configurations and handles test setup/teardown, requirement checking, and selective test execution.\n\n## Interface\n\n### Core Operations\n- `suite_with_opts/2` - Run test suite with multiple configurations\n- `run/4` - Execute single test with specific options\n- `satisfies_requirements/1` - Check test requirements\n\n## Dependencies\n\n### Direct Dependencies\n- eunit: Testing framework\n- hb_opts: Options management\n- hb_store: Storage operations\n- hb_features: Feature flags\n\n### Inverse Dependencies\n- Used by test suites\n- Testing framework\n- CI/CD integration\n\n## Implementation Details\n\n### Key Concepts\n\n1. **Test Suite Configuration**\n   ```erlang\n   % Suite configuration with options\n   suite_with_opts(Suite, OptsList) ->\n       lists:filtermap(\n           fun(OptSpec = #{ name := _Name, opts := Opts, desc := ODesc}) ->\n               Store = hb_opts:get(store, hb_opts:get(store), Opts),\n               Skip = maps:get(skip, OptSpec, []),\n               case satisfies_requirements(OptSpec) of\n                   true -> {true, {foreach, Setup, Cleanup, Tests}};\n                   false -> false\n               end\n           end,\n           OptsList\n       ).\n   ```\n   Features:\n   - Multiple configurations\n   - Store management\n   - Test filtering\n   - Requirement checking\n\n2. **Test Setup/Teardown**\n   ```erlang\n   % Test lifecycle management\n   Setup = fun() ->\n       ?event({starting, Store}),\n       hb_store:start(Store)\n   end,\n   Cleanup = fun(_) ->\n       ok\n   end\n   ```\n   Provides:\n   - Store initialization\n   - Resource cleanup\n   - Event tracking\n   - Error handling\n\n3. **Requirement Checking**\n   ```erlang\n   % Requirement validation\n   satisfies_requirements(Requirements) ->\n       lists:all(\n           fun(Req) ->\n               case hb_features:enabled(Req) of\n                   true -> true;\n                   false -> check_module_enabled(Req)\n               end\n           end,\n           Requirements\n       ).\n   ```\n   Features:\n   - Feature checking\n   - Module validation\n   - Dynamic loading\n   - Error handling\n\n### Test Management\n\n1. **Suite Organization**\n   ```erlang\n   % Test suite structure\n   Suite = [\n       {test_name, \"Test Description\", TestFunction},\n       {other_test, \"Other Description\", OtherFunction}\n   ]\n   ```\n   Manages:\n   - Test identification\n   - Test descriptions\n   - Test functions\n   - Test organization\n\n2. **Options Management**\n   ```erlang\n   % Options specification\n   OptsList = [\n       #{\n           name => config_name,\n           opts => #{key => value},\n           desc => \"Configuration Description\",\n           skip => [test_to_skip]\n       }\n   ]\n   ```\n   Handles:\n   - Configuration names\n   - Option values\n   - Test skipping\n   - Description management\n\n### Error Handling\n\n1. **Requirement Validation**\n   ```erlang\n   % Module requirement checking\n   check_module_enabled(Req) ->\n       case code:is_loaded(Req) of\n           false -> false;\n           {file, _} ->\n               case erlang:function_exported(Req, enabled, 0) of\n                   true -> Req:enabled();\n                   false -> true\n               end\n       end.\n   ```\n   Ensures:\n   - Module existence\n   - Function checking\n   - Safe validation\n   - Error prevention\n\n2. **Store Management**\n   ```erlang\n   % Safe store handling\n   Store = hb_opts:get(store, hb_opts:get(store), Opts),\n   hb_store:start(Store)\n   ```\n   Provides:\n   - Safe initialization\n   - Error handling\n   - Resource management\n   - State tracking\n\n## Integration Points\n\n1. **Test Framework**\n   - EUnit integration\n   - Test organization\n   - Result reporting\n   - Error handling\n\n2. **Store System**\n   - Store initialization\n   - Resource management\n   - State tracking\n   - Error handling\n\n3. **Feature System**\n   - Feature checking\n   - Module validation\n   - Requirement tracking\n   - Error handling\n\n## Analysis Insights\n\n### Performance Considerations\n\n1. **Resource Management**\n   - Store initialization\n   - Resource cleanup\n   - State tracking\n   - Error handling\n\n2. **Test Execution**\n   - Parallel testing\n   - Resource sharing\n   - State isolation\n   - Error containment\n\n### Security Implications\n\n1. **Resource Safety**\n   - Store isolation\n   - State protection\n   - Error containment\n   - Resource cleanup\n\n2. **Feature Safety**\n   - Feature validation\n   - Module checking\n   - Safe execution\n   - Error handling\n\n### Best Practices\n\n1. **Test Organization**\n   ```erlang\n   % Recommended test structure\n   organize_tests() ->\n       [\n           {test_name, \"Clear description\",\n               fun(Opts) ->\n                   setup_test(Opts),\n                   run_test(Opts),\n                   verify_results(Opts)\n               end\n           }\n       ].\n   ```\n\n2. **Configuration Management**\n   ```erlang\n   % Recommended configuration\n   manage_config() ->\n       #{\n           name => test_config,\n           opts => #{store => memory_store},\n           desc => \"Memory store tests\",\n           requires => [required_feature]\n       }.\n   ```\n\n3. **Test Execution**\n   ```erlang\n   % Recommended execution pattern\n   execute_tests() ->\n       Suite = organize_tests(),\n       Configs = setup_configs(),\n       suite_with_opts(Suite, Configs).\n   ```\n\n### Example Usage\n\n```erlang\n% Define test suite\nSuite = [\n    {basic_test, \"Basic functionality test\",\n        fun(Opts) ->\n            Result = perform_operation(Opts),\n            ?assertEqual(expected, Result)\n        end\n    }\n],\n\n% Define configurations\nConfigs = [\n    #{\n        name => memory_config,\n        opts => #{store => memory_store},\n        desc => \"Memory store tests\"\n    },\n    #{\n        name => disk_config,\n        opts => #{store => disk_store},\n        desc => \"Disk store tests\",\n        skip => [slow_test]\n    }\n],\n\n% Run suite with configurations\nsuite_with_opts(Suite, Configs),\n\n% Run single test\nrun(basic_test, memory_config, Suite, Configs)\n```\n\n## Test Flow\n\n```mermaid\ngraph TD\n    A[Test Suite] -->|Configure| B[Test Options]\n    B -->|Validate| C[Requirements]\n    C -->|Setup| D[Store]\n    \n    D -->|Execute| E[Test Cases]\n    E -->|Verify| F[Results]\n    F -->|Cleanup| G[Resources]\n```\n\n## Configuration Flow\n\n```mermaid\nsequenceDiagram\n    participant Test as Test Suite\n    participant Config as Configuration\n    participant Store as Store System\n    participant Exec as Execution\n\n    Test->>Config: Load Options\n    Config->>Store: Initialize Store\n    Store-->>Config: Store Ready\n    Config->>Exec: Run Tests\n    Exec->>Store: Use Store\n    Store-->>Exec: Results\n    Exec-->>Test: Test Complete\n"},"core/testing_examples/observations.md":{"content":"# Testing & Examples Observations\n\n## Architectural Patterns\n\n### 1. Test Organization\nThe testing modules demonstrate strong organizational patterns:\n\n1. **Test Suite Structure**\n   ```erlang\n   test_suite() ->\n       [\n           {test_name, \"test description\",\n               fun test_function/1}\n       ]\n   ```\n   Features:\n   - Clear naming\n   - Descriptive text\n   - Function mapping\n   - Easy maintenance\n\n2. **Configuration Management**\n   ```erlang\n   test_opts() ->\n       [\n           #{\n               name => config_name,\n               desc => \"Configuration description\",\n               opts => #{key => value},\n               skip => [tests_to_skip]\n           }\n       ]\n   ```\n   Provides:\n   - Named configs\n   - Clear descriptions\n   - Option grouping\n   - Test filtering\n\n### 2. Test Categories\n\n1. **Unit Tests**\n   ```erlang\n   % Basic unit test\n   basic_test(Opts) ->\n       Result = execute_operation(Opts),\n       ?assertEqual(expected, Result).\n   ```\n   Features:\n   - Single focus\n   - Clear assertions\n   - Easy debugging\n   - Quick feedback\n\n2. **Integration Tests**\n   ```erlang\n   % Integration test\n   integration_test(Opts) ->\n       % Setup environment\n       Environment = setup_environment(),\n       \n       % Execute operations\n       Result = execute_with_environment(Environment, Opts),\n       \n       % Verify results\n       verify_results(Result).\n   ```\n   Provides:\n   - System testing\n   - Environment setup\n   - Complex scenarios\n   - Full validation\n\n### 3. Performance Testing\nStrong performance testing approaches:\n\n1. **Benchmark Structure**\n   ```erlang\n   % Benchmark execution\n   benchmark_test() ->\n       BenchTime = 1,\n       Iterations = hb:benchmark(\n           fun() -> execute_operation() end,\n           BenchTime\n       ).\n   ```\n   Features:\n   - Time tracking\n   - Iteration counting\n   - Performance metrics\n   - Clear reporting\n\n2. **Parallel Testing**\n   ```erlang\n   % Parallel benchmark\n   parallel_test() ->\n       BenchTime = 1,\n       Workers = 16,\n       Iterations = hb:benchmark(\n           fun() -> execute_operation() end,\n           BenchTime,\n           Workers\n       ).\n   ```\n   Enables:\n   - Concurrent testing\n   - Load simulation\n   - Resource testing\n   - Scaling validation\n\n## Implementation Patterns\n\n### 1. Test Utilities\nConsistent utility patterns:\n\n1. **Test Helpers**\n   - Suite organization\n   - Option management\n   - Result validation\n   - Error handling\n\n2. **Resource Management**\n   - Environment setup\n   - State tracking\n   - Resource cleanup\n   - Error recovery\n\n### 2. Example Organization\nClear example structuring:\n\n1. **Basic Examples**\n   - Simple cases\n   - Clear purpose\n   - Easy understanding\n   - Quick validation\n\n2. **Complex Examples**\n   - Advanced scenarios\n   - Full features\n   - Real use cases\n   - Complete flows\n\n### 3. Benchmark Design\nRobust benchmark approaches:\n\n1. **Performance Measurement**\n   - Time tracking\n   - Operation counting\n   - Resource monitoring\n   - Result validation\n\n2. **Load Testing**\n   - Parallel execution\n   - Resource usage\n   - System limits\n   - Error handling\n\n## Common Themes\n\n### 1. Test Coverage\nComprehensive testing focus:\n\n1. **Functionality Coverage**\n   - Core features\n   - Edge cases\n   - Error scenarios\n   - Integration points\n\n2. **Performance Coverage**\n   - Speed testing\n   - Load testing\n   - Resource usage\n   - System limits\n\n### 2. Example Quality\nStrong example characteristics:\n\n1. **Code Quality**\n   - Clean structure\n   - Clear purpose\n   - Good practices\n   - Error handling\n\n2. **Documentation Quality**\n   - Clear descriptions\n   - Usage examples\n   - Expected results\n   - Error scenarios\n\n### 3. Benchmark Accuracy\nCareful benchmark design:\n\n1. **Measurement Accuracy**\n   - Time precision\n   - Count accuracy\n   - Resource tracking\n   - Result validation\n\n2. **Load Simulation**\n   - Real scenarios\n   - System stress\n   - Resource limits\n   - Error conditions\n\n## Areas for Improvement\n\n### 1. Documentation\nDocumentation could be enhanced:\n\n1. **Test Documentation**\n   - More examples\n   - Better descriptions\n   - Clear purposes\n   - Error scenarios\n\n2. **Example Documentation**\n   - More scenarios\n   - Better explanations\n   - Use cases\n   - Best practices\n\n### 2. Coverage\nTesting could be expanded:\n\n1. **Test Coverage**\n   - More edge cases\n   - More integrations\n   - More scenarios\n   - More validations\n\n2. **Example Coverage**\n   - More use cases\n   - More features\n   - More patterns\n   - More scenarios\n\n## Future Directions\n\n### 1. Test Enhancement\nPotential improvements:\n\n1. **Test Framework**\n   - More features\n   - Better organization\n   - Better reporting\n   - Better tools\n\n2. **Example Framework**\n   - More examples\n   - Better structure\n   - Better documentation\n   - Better tools\n\n### 2. Benchmark Enhancement\nWays to improve benchmarks:\n\n1. **Measurement Tools**\n   - More metrics\n   - Better accuracy\n   - Better reporting\n   - Better analysis\n\n2. **Load Tools**\n   - More scenarios\n   - Better simulation\n   - Better control\n   - Better reporting\n\n## Testing Architecture\n\n### 1. Component Structure\n```mermaid\ngraph TD\n    A[Testing System] -->|Unit| B[Unit Tests]\n    A -->|Integration| C[Integration Tests]\n    A -->|Performance| D[Benchmarks]\n    \n    subgraph \"Test Components\"\n        B -->|Execute| E[Test Cases]\n        B -->|Validate| F[Assertions]\n    end\n    \n    subgraph \"Integration Components\"\n        C -->|Setup| G[Environment]\n        C -->|Execute| H[Scenarios]\n    end\n```\n\n### 2. Example Structure\n```mermaid\ngraph TD\n    A[Examples] -->|Basic| B[Simple Cases]\n    A -->|Advanced| C[Complex Cases]\n    A -->|Real| D[Use Cases]\n    \n    subgraph \"Basic Examples\"\n        B -->|Show| E[Core Features]\n        B -->|Teach| F[Basic Usage]\n    end\n    \n    subgraph \"Advanced Examples\"\n        C -->|Demonstrate| G[Full Features]\n        C -->|Guide| H[Best Practices]\n    end\n```\n\n## Test Flow\n\n```mermaid\nsequenceDiagram\n    participant Test as Test Suite\n    participant Setup as Environment\n    participant Execute as Execution\n    participant Verify as Verification\n\n    Test->>Setup: Initialize\n    Setup->>Execute: Run Tests\n    Execute->>Verify: Check Results\n    Verify-->>Test: Report Status\n```\n\n## Benchmark Flow\n\n```mermaid\nsequenceDiagram\n    participant Bench as Benchmark\n    participant Workers as Worker Pool\n    participant System as System Under Test\n    participant Results as Results\n\n    Bench->>Workers: Create Workers\n    \n    loop Benchmark Time\n        Workers->>System: Execute Operations\n        System-->>Workers: Return Results\n        Workers->>Results: Record Metrics\n    end\n    \n    Results-->>Bench: Final Report\n"},"core/testing_examples/OVERVIEW.md":{"content":"# Testing & Examples System Overview\n\n## System Architecture\n\nThe Testing & Examples system provides a comprehensive framework for testing, benchmarking, and demonstrating HyperBEAM functionality. This system is built around four key components that work together to ensure system quality and provide usage examples:\n\n### 1. Test Utilities (hb_test_utils)\nThe test utilities component provides core testing infrastructure:\n\n```erlang\n% Core test suite execution\nsuite_with_opts(Suite, OptsList) ->\n    lists:filtermap(\n        fun(OptSpec = #{ name := Name, opts := Opts }) ->\n            case satisfies_requirements(OptSpec) of\n                true -> {true, setup_and_run_tests(Suite, Opts)};\n                false -> false\n            end\n        end,\n        OptsList\n    ).\n```\n\nThis enables:\n- **Suite Organization**: Structured test grouping\n- **Configuration Management**: Flexible test options\n- **Resource Management**: Clean setup/teardown\n- **Result Validation**: Clear assertions\n\nThe utilities provide:\n1. **Test Structure**\n   - Suite definition\n   - Test grouping\n   - Option handling\n   - Result management\n\n2. **Resource Handling**\n   - Environment setup\n   - State management\n   - Resource cleanup\n   - Error handling\n\n### 2. Example Framework (hb_examples)\nThe examples system demonstrates real-world usage:\n\n```erlang\n% Example test case\nrelay_with_payments_test() ->\n    % Setup test environment\n    HostWallet = ar_wallet:new(),\n    ClientWallet = ar_wallet:new(),\n    \n    % Configure and start node\n    HostNode = setup_test_node(HostWallet),\n    \n    % Execute test operations\n    execute_test_scenario(HostNode, ClientWallet).\n```\n\nThis provides:\n- **Usage Patterns**: Common use cases\n- **Integration Examples**: System interaction\n- **Error Handling**: Problem resolution\n- **Best Practices**: Recommended approaches\n\nThe framework enables:\n1. **Basic Examples**\n   - Simple scenarios\n   - Core features\n   - Quick understanding\n   - Easy validation\n\n2. **Advanced Examples**\n   - Complex scenarios\n   - Full features\n   - Real use cases\n   - Complete flows\n\n### 3. Performance Testing (hb_http_benchmark_tests)\nThe benchmark system validates performance:\n\n```erlang\n% Performance benchmark\nbenchmark_test() ->\n    BenchTime = 1,\n    BenchWorkers = 16,\n    URL = setup_benchmark_node(),\n    \n    % Execute benchmark\n    Iterations = run_benchmark(URL, BenchTime, BenchWorkers),\n    \n    % Validate results\n    validate_performance(Iterations).\n```\n\nThis enables:\n- **Speed Testing**: Operation timing\n- **Load Testing**: System stress\n- **Resource Usage**: Utilization tracking\n- **Scaling Validation**: Capacity testing\n\nThe system provides:\n1. **Performance Metrics**\n   - Operation speed\n   - Resource usage\n   - System capacity\n   - Error rates\n\n2. **Load Generation**\n   - Concurrent users\n   - Request patterns\n   - Resource stress\n   - Error scenarios\n\n### 4. Test Vectors (hb_ao_test_vectors)\nThe test vectors validate core functionality:\n\n```erlang\n% Test vector execution\ntest_suite() ->\n    [\n        {resolve_simple, \"resolve simple\",\n            fun resolve_simple_test/1},\n        {resolve_id, \"resolve id\",\n            fun resolve_id_test/1}\n    ].\n```\n\nThis provides:\n- **Functionality Testing**: Feature validation\n- **Edge Cases**: Boundary testing\n- **Error Handling**: Problem detection\n- **Integration Testing**: System interaction\n\nThe vectors enable:\n1. **Feature Testing**\n   - Core functions\n   - Edge cases\n   - Error scenarios\n   - Integration points\n\n2. **System Testing**\n   - Full flows\n   - Complex scenarios\n   - Error handling\n   - State management\n\n## System Integration\n\n### 1. Component Interaction\nThe components work together in a layered architecture:\n\n```mermaid\ngraph TD\n    A[Testing System] -->|Utils| B[Test Framework]\n    A -->|Examples| C[Example System]\n    A -->|Performance| D[Benchmark System]\n    \n    subgraph \"Test Layer\"\n        B -->|Execute| E[Test Suites]\n        B -->|Validate| F[Results]\n    end\n    \n    subgraph \"Example Layer\"\n        C -->|Show| G[Usage Patterns]\n        C -->|Guide| H[Best Practices]\n    end\n    \n    subgraph \"Performance Layer\"\n        D -->|Measure| I[Speed]\n        D -->|Test| J[Load]\n    end\n```\n\nThis enables:\n1. **Clean Architecture**\n   - Clear separation\n   - Strong cohesion\n   - Loose coupling\n   - Easy extension\n\n2. **System Coordination**\n   - Component interaction\n   - Resource sharing\n   - State management\n   - Error handling\n\n### 2. Test Flow\nThe system manages complex test flows:\n\n```mermaid\nsequenceDiagram\n    participant Test as Test Suite\n    participant Utils as Test Utils\n    participant System as System Under Test\n    participant Results as Results\n\n    Test->>Utils: Configure Test\n    Utils->>System: Execute Test\n    System-->>Utils: Return Results\n    Utils-->>Test: Report Status\n```\n\nThis provides:\n1. **Flow Management**\n   - Test execution\n   - Resource handling\n   - State tracking\n   - Result collection\n\n2. **System Coordination**\n   - Component sync\n   - Resource sharing\n   - State preservation\n   - Error propagation\n\n## Core Functionality\n\n### 1. Test Management\nThe system provides comprehensive test management:\n\n1. **Suite Organization**\n   ```erlang\n   % Test suite structure\n   organize_suite() ->\n       [\n           {test_name, \"description\",\n               fun(Opts) ->\n                   setup_test(Opts),\n                   execute_test(Opts),\n                   verify_results(Opts)\n               end\n           }\n       ].\n   ```\n\n   Features:\n   - Suite definition\n   - Test grouping\n   - Resource management\n   - Result validation\n\n2. **Configuration Management**\n   ```erlang\n   % Test configuration\n   manage_config() ->\n       #{\n           name => test_config,\n           desc => \"Test configuration\",\n           opts => #{\n               store => memory_store,\n               cache => disabled\n           }\n       }.\n   ```\n\n   Provides:\n   - Config definition\n   - Option management\n   - Resource control\n   - State handling\n\n### 2. Example Management\nThe system enables example organization:\n\n1. **Basic Examples**\n   ```erlang\n   % Simple example\n   basic_example() ->\n       % Setup\n       Environment = setup_environment(),\n       \n       % Execute\n       Result = execute_operation(Environment),\n       \n       % Verify\n       verify_result(Result).\n   ```\n\n   Features:\n   - Simple scenarios\n   - Clear purpose\n   - Easy understanding\n   - Quick validation\n\n2. **Advanced Examples**\n   ```erlang\n   % Complex example\n   advanced_example() ->\n       % Setup complex environment\n       Environment = setup_complex_environment(),\n       \n       % Execute multiple operations\n       Results = execute_operations(Environment),\n       \n       % Verify complex results\n       verify_complex_results(Results).\n   ```\n\n   Provides:\n   - Complex scenarios\n   - Full features\n   - Real use cases\n   - Complete flows\n\n## Best Practices\n\n### 1. Test Organization\nRecommended test structure:\n\n```erlang\n% Test organization pattern\norganize_tests() ->\n    % 1. Define suites\n    Suites = define_test_suites(),\n    \n    % 2. Configure options\n    Options = configure_test_options(),\n    \n    % 3. Execute tests\n    run_test_suites(Suites, Options).\n```\n\n### 2. Example Creation\nRecommended example pattern:\n\n```erlang\n% Example creation pattern\ncreate_example() ->\n    % 1. Setup environment\n    Environment = setup_example_environment(),\n    \n    % 2. Execute operations\n    Results = execute_example_operations(Environment),\n    \n    % 3. Verify and document\n    document_example_results(Results).\n```\n\n### 3. Performance Testing\nRecommended benchmark pattern:\n\n```erlang\n% Benchmark pattern\nrun_benchmark() ->\n    % 1. Configure benchmark\n    Config = configure_benchmark(),\n    \n    % 2. Execute operations\n    Results = execute_benchmark(Config),\n    \n    % 3. Analyze results\n    analyze_benchmark_results(Results).\n```\n\n## Future Directions\n\n### 1. Test Enhancement\nPotential improvements:\n\n1. **Framework**\n   - More features\n   - Better organization\n   - Better reporting\n   - Better tools\n\n2. **Coverage**\n   - More scenarios\n   - Better validation\n   - More integration\n   - More edge cases\n\n### 2. Example Enhancement\nWays to improve examples:\n\n1. **Documentation**\n   - Better explanations\n   - More scenarios\n   - Better organization\n   - Visual guides\n\n2. **Coverage**\n   - More use cases\n   - More features\n   - More patterns\n   - More integration\n\n### 3. Performance Enhancement\nAreas for improvement:\n\n1. **Benchmarks**\n   - More metrics\n   - Better accuracy\n   - Better reporting\n   - Better analysis\n\n2. **Load Testing**\n   - More scenarios\n   - Better simulation\n   - Better control\n   - Better reporting\n"}}